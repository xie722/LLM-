{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHOrGQAVkblE"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## transformer為Huggingface的套件，需要Huggingface的token授權\n",
        "- 請至huggingface網站註冊並申請token，並把token加入colab中，名為<font color=red>HF_TOKEN</font>的環境變數\n",
        "- 可參考筆記[Huggingface與Transformer套件](https://hackmd.io/@shhuangmust/HyHNqjqfJg#/)\n",
        "- 從列印出來的模型參數中，可以看到Bert的embedding token數量有30522個(類似字典裏面的單字數量)，每個token有768維的隱向量空間(Latent vector space)\n"
      ],
      "metadata": {
        "id": "s_cNCyfYCqyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "model=AutoModel.from_pretrained(\"bert-base-uncased\");\n",
        "print(model)"
      ],
      "metadata": {
        "id": "K_kwVtOckuWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bert的參數總量為109,482,240，約一億個參數"
      ],
      "metadata": {
        "id": "UqwkzYrUFFEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.num_parameters())"
      ],
      "metadata": {
        "id": "Je0PkkY26tuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "QShJoYESlWh0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}